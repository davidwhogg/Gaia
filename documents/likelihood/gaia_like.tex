\documentclass[12pt, modern]{aastex62}

\addtolength{\topmargin}{-0.25in}
\addtolength{\textheight}{0.50in}
\setlength{\parindent}{\baselineskip}

\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\Gaia}{\textsl{Gaia}}
\newcommand{\DRone}{\textsl{\acronym{DR1}}}
\newcommand{\DRtwo}{\textsl{\acronym{DR2}}}
\newcommand{\TGAS}{\textsl{\acronym{TGAS}}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\equationname}{equation}

\newcommand{\AU}{\mathrm{A.U.}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\inv}{^{-1}}

\shorttitle{a likelihood for gaia}
\shortauthors{david w hogg}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\noindent
\title{A likelihood function for the \Gaia\ Data\footnote{%
  Copyright 2018 the author. Feel free to reproduce and redistribute, provided
  that you make no changes whatsoever.}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}
\affil{Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\begin{abstract}\noindent
When we perform probabilistic inferences with the \Gaia\ Mission data,
we technically require
a \emph{likelihood function}, or a probability of the (raw-ish) data as a function
of stellar (astrometric and photometric) properties.
Unfortunately, we aren't (at present) given access to the \Gaia\ data
directly;
we are only given a catalog of derived astrometric properties for the stars.
How do we perform probabilistic inferences in this context?
The answer---implicit in many publications---is that we should look at the
\Gaia\ Catalog as containing the \emph{parameters of a likelihood function}, or
a probability of the \Gaia\ data, conditioned on stellar properties,
evaluated at the location of the data.
Concretely, my recommendation is to assume
(for, say, the parallax) that the Catalog-reported
value and uncertainty are the mean and root-variance of a Gaussian
function that can stand in for the true likelihood function.
This is the implicit assumption in most \Gaia\ literature to date;
my only goal here is to make the assumption explicit.
Certain technical choices by the Mission team slightly invalidate
this assumption for \DRone\ (\TGAS), but not seriously. Generalizing beyond \Gaia,
it is important to downstream users of any catalog products
that they deliver likelihood information about the fundamental data;
this is a challenge for the probabilistic catalogs of the future.
\end{abstract}

\keywords{foo --- bar}

\section*{~}\clearpage
\section{Introduction}
The age of \Gaia\ is also (perhaps coincidentally) the age of principled
probabilistic inference in astrophysics.
For this reason, the \Gaia\ (\citealt{gaia}) data are being used in many probabilistic
inferences (for example, \citealt{tri3, hawkins, sesar}).
These probabilistic inferences take many forms, and have different levels
of hierarchical complexity, but all of them require that there be, at base,
a likelihood function, or a probability for the \Gaia\ data as a function
of model parameters.
One confusing question investigators face is: What constitutes the \Gaia\ data?
And how do I write a probability over it, when all I get to see is the
official Catalog release, with photometric and astrometric parameters and associated
uncertainties?

There is a standard answer, but in most inferences it appears only implicitly:
The inferences presume that the \Gaia\ Catalog entries can be used to construct
a likelihood function approximation, which is appropriate for use in inferences.
In all work so far, this likelihood function has been given a Gaussian form,
with mean and variance set to Catalog values (for example, this is clearly
stated in the introductory remarks in \citealt{tri2}).
The implicit assumptions underlying this choice are
what I am attempting to make explicit in this \documentname.

An investigator can take one of (at least) two attitudes towards the \Gaia\ data:
The investigator can think of the \Gaia\ Catalog as being the data, in which case
the assumption is that the generative process for the \Gaia\ Catalog entries is
itself Gaussian.
Or the investigator can think of the \Gaia\ data as taking some raw form
from which the Catalog has been derived (by, say, pipelines),
in which case the Catalog contains parameters of a Gaussian
approximation to the likelihood function for those raw data.
It turns out it doesn't matter which attitude the investigator takes; the
proposal for the \Gaia\ likelihood function made here works in either case;
the two attitudes are identical in the limit that the Catalog contains
\emph{sufficient statistics} of the raw data.
Indeed, it is almost a definition of sufficient statistics that---if you have them---you
can use them to construct a good approximation to the likelihood function for the
raw data.
All that said, we are going to take the latter attitude: That is, that the
\Gaia\ data are raw data, and the Catalog is delivering statistics that can
be used to construct an approximation to the likelihood function.

One contemporary trend in astrophysics is to think about replacing rigid
catalogs with something more probabilistic, possibly representing uncertainties
through a sampling in catalog space (for example, \citealt{brewer, portillo}).
This idea is also informing some of the expected high-level outputs from
the \Gaia\ Mission too (\citealt{apsis}).
These ideas are interesting and new, and connect to the age of principled
probabilistic inference in which we find ourselves.
However, these ideas also come with substantial risk:
In many cases, a posterior sampling or posterior probability information
does not successfully encode sufficient likelihood information to permit
downstream analyses (with, say, different priors).
That is, investigators generally want---from an experiment or data source---likelihood
information, not posterior information.
This is because different investigators can have very different priors,
even qualitatively different priors; they won't agree on anything about the
data except what new information those data bring.
This all flows from two principles: The first is the likelihood principle,
which states that new knowledge comes in likelihood form.
The second is the subjectivity of inference, or the principle that
\emph{an experiment ought to produce likelihood updates for all investigators,
no matter what their prior beliefs}.

\section{A likelihood function for \textsl{Gaia}}
In the simplest possible case, imagine that we are trying to infer
the true\footnote{There are many possible meanings for the word ``true''.
  In this context, we say the ``true distance'' because it is not the measured
  distance, but rather the distance that the star truly has in some model.}
distance $d_n$ to a star $n$ given the \Gaia\ data $y_n$ that
pertain to star $n$, and nothing else
(which is the goal, for example, of \citealt{tri2})
The inference looks like this:
\begin{eqnarray}
p(d_n\given y_n) = \frac{1}{Z_n}\,p(y_n\given d_n)\,p(d_n)
\label{eq:inference}
\quad ,
\end{eqnarray}
where
$p(d_n\given y_n)$ is the posterior pdf for the true distance given the data,
$Z_n$ is a normalization constant,
$p(y_n\given d_n)$ is the likelihood (or the pdf for the data given the distance),
and $p(d_n)$ is the prior pdf for the true distance.
In order to perform this inference, we need the likelihood and the prior.

Note that, in order to write down these schematic equations,
we don't need to be perfectly specific here about what, exactly,
is the data $y_n$.
It could be all the raw \Gaia\ data pertaining to this particular star $n$,
or it could be the catalog entry in the \Gaia\ Catalog on star $n$.
Nothing about this formalism changes with these different choices,
although there might be some implicit marginalizations over nuisance
parameters in some of these particular cases.
That is, what you explicitly put in for the likelihood function $p(y_n\given d_n)$
will depend on what you consider to be the data $y_n$, but the formal structure
will be identical.

My goal here is to promote a particular choice for this likelihood function.
Getting straight to the point, in this simplest possible case, 
\begin{eqnarray}
p(y_n\given d_n) &=& p(y_n\given\varpi_n)
\label{eq:gotoparallax}
\\
\varpi_n &\equiv& \frac{1\,\AU}{d_n}
\\
p(y_n\given\varpi_n) &=& A_n\,N(\varpi_n\given\hat{\varpi}_n,\hat{\sigma}^2_{\varpi n})
\label{eq:onedlike}
\quad ,
\end{eqnarray}
where
$\varpi_n$ is the true parallax to star $n$ at true distance $d_n$ (implicitly $\varpi_n$ is measured in radians here),
$p(y_n\given\varpi_n)$ is the likelihood as a function of true parallax (rather than distance),
$A_n$ is a normalization (and units-conversion) constant,
$N(\xi\given\mu,V)$ is the Gaussian function for $\xi$ given mean $\mu$ and variance $V$,
$\hat{\varpi}_n$ is the (noisy) value given for the parallax of star $n$ in the \Gaia\ Catalog,
and $\hat{\sigma}_{\varpi n}$ is the value given for the uncertainty on that parallax.
The amplitude $A_n$ is not directly given in the \Gaia\ Catalog
but it turns out---because of that factor of $1/Z$ in
\equationname~(\ref{eq:inference})---it isn't needed for parameter-estimation-like
inferences.
If you \emph{do} find that you are doing an inference for which the amplitude
$A_n$ matters---for instance some inferences that might involve comparing certain
kinds of fully marginalized likelihoods---then $A_n$ can probably be reconstructed
from a goodness-of-fit statistic in the Catalog (that we expect in \DRtwo).

It is more stable numerically to do inferences with log probabilities.
In the log,
\begin{eqnarray}
\ln p(y_n\given\varpi_n) &=& Q_n - \frac{1}{2}\,\frac{[\varpi_n - \hat{\varpi_n}]^2}{\hat{\sigma}^2_{\varpi n}}
\\
Q_n &\equiv& \ln\frac{A_n}{\sqrt{2\pi\,\hat{\sigma}^2_{\varpi n}}}
\quad .
\end{eqnarray}

This choice (\ref{eq:onedlike}) for the likelihood function is Gaussian,
and presumes that the
Catalog values for the parallax and its uncertainty are accurate and represent
a likelihood maximum and width.
It is the choice made in many publications (CITE).
It has the property that the likelihood peaks when the true parallax
matches the Catalog-reported parallax,
and that the function is symmetric in parallax space---not distance space---because
\Gaia\ measures geometric parallaxes, not distances.
The likelihood function is a pdf for the data, evaluated at the data (which,
for a Bayesian, are fixed), and therefore although it is a pdf over data,
it is really a function of the true parallax:
The likelihood function (\ref{eq:onedlike}) returns the answer to the question:
How probable are the observed data, if $\varpi_n$ is the true parallax of
star $n$?

It is worth making a few technical notes related to dimensions or units:
Because the data $y_n$ and
the true parallax $\varpi_n$ have (in general, though not always) different units,
the unknown amplitude $A_n$ will have (in general) non-trivial units.
Also, you might think
that the conversion from distance $d_n$ to parallax $\varpi_n$
in \equationname~(\ref{eq:gotoparallax})
would bring in some Jacobian factors of the form $||\dd d_n/\dd\varpi_n||$.
However, because the true distance only parameterizes a function of the data,
or because the likelihood function has units of per-data (and not per-parallax),
the change of parameters doesn't bring a change of units, at least not in the
likelihood function (it would bring a change of units in the posterior pdf, or the
prior pdf).
I say more about these units issues elsewhere (\citealt{calculus}).

In a more general inference, it is not just the parallax (or distance)
that the investigator is modifying, but the (say) $D=5$ astrometric quantities
(celestial positions, proper motions, and parallax)
or a non-trivial subset of $D$ of these.
In this case the likelihood becomes
\begin{eqnarray}
p(y_n\given X_n) &=& A_n\,N(X_n\given\hat{X}_n,\hat{C}_{Xn})
\label{eq:like}
\quad ,
\end{eqnarray}
where now
$X_n$ is a $D$-vector of true values for the astrometric quantities for star $n$,
$p(y_n\given X_n)$ is the likelihood as a function of that vector of true quantities,
$A_n$ is again a normalization (and units-conversion) constant,
$N(\xi\given\mu,V)$ is now the Gaussian function for $D$-vector $\xi$ given mean $D$-vector $\mu$ and $D\times D$ covariance matrix $V$,
$\hat{X}_n$ is the $D$-vector of values given for the $D$ astrometric quantities for star $n$ in the \Gaia\ Catalog,
and $\hat{C}_{Xn}$ is the value given for the $D\times D$ covariance matrix for that
$D$-vector.
Again, the assumption here is that the likelihood function has Gaussian form, peaked
when the true values match the Catalog values, and symmetric in the quantities
reported in the Catalog (which are celestial positions, proper motions, and parallaxes).
In the log, this is
\begin{eqnarray}
\ln p(y_n\given X_n) &=& Q_n - \frac{1}{2}\,[X_n - \hat{X}_n]\T\cdot\hat{C}_{Xn}\inv\cdot [X_n - \hat{X}_n]
\\
Q_n &\equiv& \ln\frac{A_n}{\sqrt{||2\pi\,\hat{C}_{Xn}||}}
\quad ,
\end{eqnarray}
where we have implicitly assumed that the $D$-vectors are column vectors.

The likelihood functions of (\ref{eq:onedlike}) and (\ref{eq:like}) are
probabilities of the observed \Gaia\ data as a
function of astrometric parameters.
But there are many more parameters, including photometric, point-spread-function,
and spacecraft-attitude parameters as well, all of which contribute to the probability
for the data.
How do we deal with these nuisance parameters, or how can we ignore them?
Implicitly, I am assuming here that the \Gaia\ Catalog team has either optimized
or marginalized out these nuisance parameters.
Since the \Gaia\ Mission delivers immense signal-to-noise on these nuisance
parameters in most cases, it won't matter much, technically, to this
discussion whether they optimize
the nuisance parameters or marginalize them out.

Of course, most inferences are not as simple as the inference described by
\equationname~(\ref{eq:inference}).
Except in rare cases, we aren't just trying to find out the distance to a single
star!
We usually are trying to fit some model of the world, or of some set of stars,
or calibrate a color-luminosity relationship, or something like that (CITE VARIOUS).
In these cases, there is a model $p(X_n\given\theta)$ which says what we expect
for star $n$'s true astrometric quantites $X_n$, given a set of parameters $\theta$
of our larger model.
Within that larger model, the likelihood for a single star becomes
\begin{eqnarray}
p(y_n\given\theta) &=& \int p(y_n\given X_n)\,p(X_n\given\theta)\,\dd X_n
\quad ,
\end{eqnarray}
where we have marginalized out the true astrometric properties $X_n$ for star $n$.
These marginalized likelihoods $p(y_n\given\theta)$
can be multiplied together (or, better, added in the log)
to make likelihoods for collections of stars.
None of that hierarchical structure or marginalization changes the story here;
which is that the internal likelihood $p(y_n\given X_n)$ should be given the
Gaussian form in \equationname~(\ref{eq:like}).

HOGG: Deterministic vs stochastic parameters.

\section{Discussion}
In order to perform inferences
with the \Gaia\ data, we need a likelihood function.
The main point of this \documentname\ is that a sensible likelihood function
stand-in or surrogate can be constructed from the \Gaia\ Catalog, under the assumption
that the Catalog contains likelihood information, and accurate (and sufficient)
statistics of the data.
I give explicit forms for the likelihood function surrogate in
\equationname s~(\ref{eq:onedlike}) and (\ref{eq:like}).
These likelihood functional forms are not new---they are used in multiple places
in the literature (for example, CITE)---the point of this is to make the
likelihood function assumptions explicit.

HOGG: As we said above: Can think of this as a LF given the catalog data. Or of the raw data.
These will be identical if the catalog contains a set of sufficient statistics
for the raw data!
Attitude taken here is that we care about the raw data likelihood.

HOGG: What have we assumed? That the \Gaia\ \DPAC\ did and will do LF optmimization.
And the
noise is close to Gaussian, and the signal-to-noise is high. These are wrong for
\TGAS\ (\citealt{michalik, dr1}), but the prior was so broad, it won't matter in practice (and explain how using the LF from Tycho as the prior for Gaia is a flat prior). TECHNICALLY, you should use
the ratio of the Gaussian in \equationname~(\ref{eq:onedlike}) or (\ref{eq:like}) with
a ratio of this Gaussian to the prior Gaussian, which is far broader. That would be
a difference in log space.

HOGG: What about star--star covariances? These are being ignored too, but they
will be small in the long run (\citealt{holl}).

HOGG: And, relatedly, can we really split the data into disjoint $y_n$ subsets?
No!

\emph{What about non-Gaussianity?} Everything written here
 explicitly assumes that the \Gaia\ noise
model is correct and that the noise is entirely Gaussian.
That's probably a bad assumption in detail,
especially as there are cosmic rays and there is binary (and triple)
contamination and stellar blending and crowding and so on.
However, it is the standard assumption across \Gaia\ science at present (CITE THINGS).

One thing I like to say about Gaussianity is that if you are given a mean
and a variance for a distribution, and you believe those statistics to be
accurate, then the Gaussian is the \emph{most conservative} assumption one
can make.
It is the maximum-entropy distribution with that mean and variance!
Of course this point is something of a red herring, because it is precisely
when the noise is non-Gaussian that it will be impossible to obtain an
accurate estimate of the mean and variance.
The point here is that the Gaussianity assumption in this work is strongly connected
to the assumption that the \Gaia\ Catalog is delivering accurate statistics of
the data.

HOGG: What about binaries and so on? Hard to see how the \Gaia\ Collaboration can
maintain the likelihood framework in this case...

HOGG: Why are there negative parallaxes? There will be, and you can think about
it two ways: Linear fitting will do this at low s/n in general. And the
LF approximation might require it!
They are required if the team is delivering likelihood information.

HOGG: What are we doing with our inferences? If we are producing true posterior
information (as with CITE this and CITE that), then our inferences may not
be re-usable by anyone. This is a danger for all of us. And it isn't enough
to report our priors, because: Reproducibility, support, etc.

\acknowledgements
It is a pleasure to thank
  Lauren Anderson (Flatiron),
  Coryn Bailer-Jones (MPIA),
  Berry Holl (Geneva),
  Boris Leistedt (NYU),
  Lennart Lindegren (Lund),
  Hans-Walter Rix (MPIA),
and the Gotham \Gaia\ community for help with this project.
This work was partially supported by
  the National Science Foundation (grant \acronym{AST-1517237}).

\bibliography{gaia}

\end{document}
